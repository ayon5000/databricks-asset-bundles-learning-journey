Contains timestamps/notes recorded during first pass of learning. This may or may not align with the structure of the course available today. In other words, if significant time has elapsed between the max(Date) and date of viewing, this file might not be aligned with the course.

Date | Section | Video Name | Timestamp | Concept | Code/Tool | Notes |
| -- | -- | -- | -- | -- | -- | -- |
2025-08-30 | 2 | Azure Databricks Workspace Setup | [1:42] | Azure DB resource group and workspace setup for all 3 environments | | |
2025-08-30 | 2 | Azure Databricks Workspace Setup | [4:30] | Same UC metastore created for all 3 workspaces in the same region  | | |
2025-08-30 | 2 | VS Code | [1:05] | Databricks extension installation in VS Code | VS Code |  |
2025-08-30 | 2 | VS Code: Tutorial | N/A | Link to VS Code guide Includes some shortcuts that can be reviewed | VS Code | [Link](https://code.visualstudio.com/docs/introvideos/basics) |
2025-09-01 | 2 | Local Java Installation | All | Spark / Java Compatibility, Mapping DB Runtime to specific Java version required | Check java version installed: java --version | - To work with Databricks locally, you will need Java installed.<br/> - Go to [Link](https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/) to check the current LTS version (check non-ML) and the version of Spark associated with it. <br/>- Once you have identified the Spark version, go to this [Link](https://community.cloudera.com/t5/Community-Articles/Spark-and-Java-versions-Supportability-Matrix/ta-p/383669) to check Spark-Java version mapping and identify the java version that needs to be installed.<br/> -In case the Spark-Java mapping is not available, go to the [Spark Homepage](https://spark.apache.org/docs/3.5.2/) for the specific version (***make sure you change the Spark version in the link***) and get the Java version for installation.<br/> - [Java Downloads Page](https://www.oracle.com/java/technologies/downloads/?er=221886)<br/> - **NOTE:** When we talk about Java Installation, we are talking about JDK installation, not JRE installation.<br/> - **Example:** Latest LTS DBR (Databricks Runtime) as of today is Databricks Runtime 16.4 LTS ML. The Databricks Runtime 16.4 LTS is powered by powered by Apache Spark 3.5.2, but we are unable to locate the corresponding Java version at [Link](https://community.cloudera.com/t5/Community-Articles/Spark-and-Java-versions-Supportability-Matrix/ta-p/383669). So, we look at [Spark Homepage](https://spark.apache.org/docs/3.5.2/) to find out in the Downloads section that "Spark runs on Java 8/11/17".<br/>  - In case you multiple versions of Java installed? - Set PATH for relevant version |
2025-09-01 | 2 | Python Installation | All | Python Installation for working with Databrciks on Local System | Python | You need to install the Python version associated with the Databricks runtime. In order to check the Python version required, go to the [Microsoft page listing the DBRs](https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/), go to the specific DBR, on the right you will find a section called 'System environment', check on it and you should be able to see the Python version listed. Please note that we just need to align the major and minor version i.e. Databricks Runtime 16.4 LTS is associated with Python: 3.12.3. Having Python: 3.12.x installed will suffice where x is a non-negative integer |
2025-09-01 | 2 | Databricks CLI | [0:15] | - Install Databricks CLI using curl<br/> - Current curl command for installation: ```curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh \| sh``` | To check Databricks CLI version:<br/> - databricks --version | - Check this [link](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/install) for installation with curl.<br/> - For Linux and macOS, if an error message states that /usr/local/bin is not writable, then run the command again with sudo i.e. add sudo infront of each piped command e.g. ```sudo curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh \| sudo sh``` |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [1:18] | Authenticate DB CLI installed locally with AZ DB workspace so that we can interact with it from our local computer | Useful commands:<br/> - databricks -h<br/> - databricks configure -h (allows us to configure CLI to interact with databricks) | - databricks configure --host <db dev workspace url> --profile DEFAULT (Command to configure 'DEFAULT' profile)<br/> - databricks auth profiles (Lists all of the profiles configured)<br/> - databricks auth describe (Gives details of how we have configured profile)<br/> - When we create a profile, a .databrickscfg file is created in our local system's user profile folder e.g. C:\Users\username\.databrickscfg. This file contains the profile details including the PAT used for authentication. |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [2:56] | PAT creation for DB Workspace  |  |  |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [5:50] | Alternate method to open .databrickscfg file and add TEST profile  | VS Code Databricks Extension | - databricks auth describe --profile TEST (to describe a specific profile created - Generally we will created only DEV/DEFAULT profile locally since we will deploy to TEST and PROD automatically) |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | N/A | Databricks CLI authentication method on Azure  |  | - [All DB CLI Auth Methods for AZ DB](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication)<br/> - [Authentication with Service Principal](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication#azure-sp-auth) |
2025-09-01 | 3 | Databricks CLI | N/A | Why you need to know the Databricks CLI? | DB CLI | Why you need to know the Databricks CLI? <br/>- Databricks Asset Bundles are a feature of the Databricks CLI. You build bundles locally, then use the Databricks CLI to deploy your bundles to target remote Databricks workspaces and run bundle workflows in those workspaces from the command line.<br/> - The CLI’s bundle commands output structured JSON and support authentication profiles, making them essential for scripting bundle workflows in CI/CD pipelines—without it, you can’t automate builds, tests or deployments of your Databricks assets |
2025-09-01 | 3 | Databricks CLI Demo | [1:11] | Create DB Cluster with CLI | - ```databricks clusters create --json @create-compute.json --profile DEFAULT``` <br/> - ```databricks clusters create --json @create-compute.json --profile TEST``` (You may get an error when executing this command using the same JSON. In this case the error is because the profile_id key in the JSON has a value associated with DEFAULT workspace and not the TEST workspace/profile ) <br/> - ```databricks clusters list --profile TEST``` (Listing clusters in TEST profile/workspace along with the cluster id and cluster state)<br/> - ```databricks clusters delete xxx-xxx-xxx``` (To TERMINATE the cluster. Here, xxx-xxx-xxx is a cluster id)<br/> - ```databricks clusters permanent-delete xxx-xxx-xxx``` (To Permanently detele the cluster using the cluster id)<br/> - ```databricks clusters permanent-delete xxx-xxx-xxx --profile TEST``` (To Permanently detele a cluster using the cluster id and profile information. By default, the cluster is searched in the workspace in the DEFAULT profile) | Create DB Cluster with CLI, also recommended method of using a JSON file as the request body (i.e. start creating the cluster fron UI and copy the JSON to invoke via DB CLI on your local system.). |
2025-09-01 | 3 | Databricks CLI Demo | [11:17] | Obtain cluster ID from Databricks UI |  |  |
2025-09-01 | 3 | Databricks CLI Demo | N/A | Databricks CLI commands reference | Databricks CLI | [Databricks CLI commands](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/commands) |
2025-09-01 | 3 | Databricks CLI Demo | N/A | Databricks REST API reference | Databricks CLI | [Databricks REST API reference](https://docs.databricks.com/api/azure/workspace/introduction) |
2025-09-01 | 3 | Databricks CLI Demo | N/A | How Databricks CLI works | Databricks CLI | When we execute a DB CLI command, the DB REST API is invoked under the hood. |
2025-09-01 | 4 | Initialising our Bundle Project | [1:05] | How to initiate Databricks Asset Bundle project and provide an overview of the bundles. | - ```databricks bundle init -h```<br/>- ```databricks bundle init --profile DEFAULT```<br/> | |
2025-09-01 | 4 | Initialising our Bundle Project | [3:07] | Project folder structure, and main configuration file. | | - databricks.yml file is the blueprint of project.Each project should only have one databricks.yml file. <br/>- When you run DAB, it takes yaml, coverts to json, and calls DB rest API.<br/>- Additional configutaions is specfiied via include key in yaml file and for the project chosen, they are in the resources folder.<br/>- scratch folder not included for production/bundle deployment. It is ignored via the .gitignore file.It isintended to be used as an exploration space. |
2025-09-01 | 4 | Initialising our Bundle Project | N/A | YAML Syntax | YAML | [YAML Syntax](https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html) |
2025-09-01 | 4 | Initialising our Bundle Project | N/A | DAB Configuration | | [DAB Configuration](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/settings) |
2025-09-01 | 4 | Initialising our Bundle Project | N/A | DAB Configuration Reference | | [DAB Configuration Reference](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/reference) |
2025-09-01 | 4 | Validating and Deploying the bundle | [0:13] | Validate Bundle before deployment | - ```databricks bundle validate``` | - By default, for root_path the Databricks CLI uses the default path of ```/Workspace/Users/${workspace.current_user.userName}/.bundle/${bundle.name}/${bundle.target}```, which uses substitutions. <br/> Here workspace.current_user.userName = databricks user name. ${bundle.name} is the name of the bundle (e.g. db_project) and ${bundle.target} is dev/test/prod etc. |
2025-09-01 | 4 | Validating and Deploying the bundle | [2:42] | Deploy Bundle | - ```databricks bundle deploy``` | root_path is where the bundle file are uploaded to in the databricks workspace. Post successful execution of the deploy command, navigate to the "root_path" in the development workspace. You will see 3 folders:<br/> - artifacts: stores any build artifacts such as Python packages. So for now it's empty because we don't have any build artifacts. <br/> - files: Stores all the files. So this is all the code files. Notice how we have the resources and the fixtures directories as well as the Databricks YAML and the README.md file.But we have not got the other folders like scratch and .databricks (they were excluded using the .gitignore). <br/> - state: used to track metadata about your previous deployments. When you redeploy a bundle, the tool compares your current configuration to the previous state and incrementally updates only the changed assets and then refreshes the state file to ensure that everything in your Databricks workspace stays in sync. |
2025-09-01 | 4 | Validating and Deploying the bundle | [5:06] | Adding a new folder to the bundle. |  |  |
2025-09-01 | 4 | Validating and Deploying the bundle | [6:45] | For viewing summary but not doing any validation. | - ```databricks bundle summary```  |  |
2025-09-01 | 4 | Validating and Deploying the bundle | [7:00] | Adding a custom root_path for dev. |  |  |
2025-09-01 | 4 | Deploying the bundle to different targets | [0:20] | Deploying the bundle to different targets | ```databricks bundle deploy --target test``` | Update the databricks.yml file to define other targets and update the root_path and workspace host as shown. Update the .databrickscfg file to include information about any missing targets. Earlier, when we executed ```databricks bundle deploy``` , we deployed only the 'dev' target because in databricks.yml, under targets->dev, we set ```default: true```. Please note that the 'mode' for both 'test' and 'prod' targets is 'production' while that of the 'dev' target is 'development' |
2025-09-01 | 4 | Deploying the bundle to different targets | [5:40] | Attempt to deploy dev target in the Shared path | | - We get an error. The root path must contain the current username to ensure uniqueness when using the mode as 'development'. So when the mode is 'development', it does not allow you to deploy to a non-unique location. You must have your username in the deployment route path. So, picture this scenario when multiple developers are making changes in their development cycle, the last thing you want for them to do is keep overwriting each other's changes. So that's why when the mode for the deployment is set to development, then the path must contain the username. <br/> - For 'test' and 'prod' targets, we can set the mode to 'production' because we don't have a requirement to deploy to a unique location, because all assets for testing and production should be deployed to a centralized, shared location where the changes have been thoroughly tested and approved. So the deployments to test and prod would normally be more controlled than deployments to Dev. <br/> - **Best practice to remove the root_path from dev target (with mode: development) and deploy it in the default unique location.** <br/> - **In a real world project from your local workspace, you should only be deploying to the dev target. But for the purposes of this demo, we wanted to show you how you can deploy to multiple targets such as test and prod. This is so you'll understand later on in this course how to set up a CI CD pipeline that will deploy to test and prod automatically.** |
2025-09-01 | 4 | Simple Workflow Job Demo and Deployment Modes | [0:06] | How to define and deploy a job via Databricks Asset bundles. |  | - The job name has a prefix against the job name. Let's discuss why we have this prefix here. But before we do that, let's actually deploy this to the test workspace. So currently we don't have any workflows here. So now let's deploy it to the test workspace. So for the test target we're deploying the bundle in the shared folder. We're not deploying it in our workspace folder. In test target, if we look at the job name, we see that, the workflow doesn't have a prefix. So this behavior is controlled by the 'mode' key in the target specification. In our Databricks YAML setup you'll recall that for the target of 'dev' we've specified the mode is 'development', but the mode is 'production' for 'test'. So when the mode is set to development, a prefix which includes the username is automatically added to the job name. This ensures that when multiple developers deploy their jobs in the development environment, their deployments remain unique and don't overwrite each other. In contrast, this prefix isn't required when the mode is set to production. When the mode is production, we aim to have a single consolidated workflow for either test or production use cases.<br/> - Further details can be found here: [Databricks Asset Bundle deployment modes \| Development mode](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/deployment-modes#development-mode)  |
2025-09-01 | 4 | Simple Workflow Job Demo and Deployment Modes | [6:47] | Custom presents to define prefix when deploying in 'development' mode. |  | - [Custom presets](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/deployment-modes#custom-presets) <br/> - Please note that the custom prefix must be unique when mode='development' for a target. |
2025-09-01 | 4 | Modularising your configuration files | [0:27] | The configuration files are added directly to this databricks.yml file. Now, having all of the configuration in a single file can get quite complex. If we had multiple 'resources', then the file can get pretty long and it can get difficult to manage. So it's a good idea to modularize the configuration. |  | Notes |
2025-09-01 | 4 | Destroying a bundle | [0:00] | Destroying a bundle | - ```databricks bundle destroy``` | The Databricks bundle destroy command permanently deletes a bundles previously deployed assets such as jobs, pipelines, and artifacts. This action cannot be undone in the dev workspace. |
2025-09-01 | 4 | How Databricks tracks the state of your Deployments  | [0:35] | Deployment State Tracking |  | - How Databricks keeps track of your various deployments and their state. So the .databricks directory in the root folder of your bundle acts as a manifest. It contains files that store detailed logs and tracks the state of every asset in your bundle. The Databricks CLI will compare the current state of your local assets with the last known deployed state recorded in this directory. And if the file remains unchanged since the last deployment, then the tool will recognize that there's no need to redeploy it, which speeds up the process by updating only the assets that have changed. <br/> - In order to force a full deployment, delete the .databricks folder and deploy again. |