Contains timestamps/notes recorded during first pass of learning. This may or may not align with the structure of the course available today. In other words, if significant time has elapsed between the max(Date) and date of viewing, this file might not be aligned with the course.

Date | Section | Video Name | Timestamp | Concept | Code/Tool | Notes |
| -- | -- | -- | -- | -- | -- | -- |
2025-08-30 | 2 | Azure Databricks Workspace Setup | [1:42] | Azure DB resource group and workspace setup for all 3 environments | | |
2025-08-30 | 2 | Azure Databricks Workspace Setup | [4:30] | Same UC metastore created for all 3 workspaces in the same region  | | |
2025-08-30 | 2 | VS Code | [1:05] | Databricks extension installation in VS Code | VS Code |  |
2025-08-30 | 2 | VS Code: Tutorial | N/A | Link to VS Code guide Includes some shortcuts that can be reviewed | VS Code | [Link](https://code.visualstudio.com/docs/introvideos/basics) |
2025-09-01 | 2 | Local Java Installation | All | Spark / Java Compatibility, Mapping DB Runtime to specific Java version required | Check java version installed: java --version | - To work with Databricks locally, you will need Java installed.<br/> - Go to [Link](https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/) to check the current LTS version (check non-ML) and the version of Spark associated with it. <br/>- Once you have identified the Spark version, go to this [Link](https://community.cloudera.com/t5/Community-Articles/Spark-and-Java-versions-Supportability-Matrix/ta-p/383669) to check Spark-Java version mapping and identify the java version that needs to be installed.<br/> -In case the Spark-Java mapping is not available, go to the [Spark Homepage](https://spark.apache.org/docs/3.5.2/) for the specific version (***make sure you change the Spark version in the link***) and get the Java version for installation.<br/> - [Java Downloads Page](https://www.oracle.com/java/technologies/downloads/?er=221886)<br/> - **NOTE:** When we talk about Java Installation, we are talking about JDK installation, not JRE installation.<br/> - **Example:** Latest LTS DBR (Databricks Runtime) as of today is Databricks Runtime 16.4 LTS ML. The Databricks Runtime 16.4 LTS is powered by powered by Apache Spark 3.5.2, but we are unable to locate the corresponding Java version at [Link](https://community.cloudera.com/t5/Community-Articles/Spark-and-Java-versions-Supportability-Matrix/ta-p/383669). So, we look at [Spark Homepage](https://spark.apache.org/docs/3.5.2/) to find out in the Downloads section that "Spark runs on Java 8/11/17".<br/>  - In case you multiple versions of Java installed? - Set PATH for relevant version |
2025-09-01 | 2 | Python Installation | All | Python Installation for working with Databrciks on Local System | Python | You need to install the Python version associated with the Databricks runtime. In order to check the Python version required, go to the [Microsoft page listing the DBRs](https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/), go to the specific DBR, on the right you will find a section called 'System environment', check on it and you should be able to see the Python version listed. Please note that we just need to align the major and minor version i.e. Databricks Runtime 16.4 LTS is associated with Python: 3.12.3. Having Python: 3.12.x installed will suffice where x is a non-negative integer |
2025-09-01 | 2 | Databricks CLI | [0:15] | - Install Databricks CLI using curl<br/> - Current curl command for installation: ```curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh \| sh``` | To check Databricks CLI version:<br/> - databricks --version | - Check this [link](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/install) for installation with curl.<br/> - For Linux and macOS, if an error message states that /usr/local/bin is not writable, then run the command again with sudo i.e. add sudo infront of each piped command e.g. ```sudo curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh \| sudo sh``` |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [1:18] | Authenticate DB CLI installed locally with AZ DB workspace so that we can interact with it from our local computer | Useful commands:<br/> - databricks -h<br/> - databricks configure -h (allows us to configure CLI to interact with databricks) | - databricks configure --host <db dev workspace url> --profile DEFAULT (Command to configure 'DEFAULT' profile)<br/> - databricks auth profiles (Lists all of the profiles configured)<br/> - databricks auth describe (Gives details of how we have configured profile)<br/> - When we create a profile, a .databrickscfg file is created in our local system's user profile folder e.g. C:\Users\username\.databrickscfg. This file contains the profile details including the PAT used for authentication. |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [2:56] | PAT creation for DB Workspace  |  |  |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [5:50] | Alternate method to open .databrickscfg file and add TEST profile  | VS Code Databricks Extension | - databricks auth describe --profile TEST (to describe a specific profile created - Generally we will created only DEV/DEFAULT profile locally since we will deploy to TEST and PROD automatically) |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | N/A | Databricks CLI authentication method on Azure  |  | - [All DB CLI Auth Methods for AZ DB](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication)<br/> - [Authentication with Service Principal](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication#azure-sp-auth) |
2025-09-01 | 3 | Databricks CLI | N/A | Why you need to know the Databricks CLI? | DB CLI | Why you need to know the Databricks CLI? <br/>- Databricks Asset Bundles are a feature of the Databricks CLI. You build bundles locally, then use the Databricks CLI to deploy your bundles to target remote Databricks workspaces and run bundle workflows in those workspaces from the command line.<br/> - The CLI’s bundle commands output structured JSON and support authentication profiles, making them essential for scripting bundle workflows in CI/CD pipelines—without it, you can’t automate builds, tests or deployments of your Databricks assets |
2025-09-01 | 3 | Databricks CLI Demo | Timestamp | Concept | Code/Tool | Notes |