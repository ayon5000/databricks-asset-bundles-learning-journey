Contains timestamps/notes recorded during first pass of learning. This may or may not align with the structure of the course available today. In other words, if significant time has elapsed between the max(Date) and date of viewing, this file might not be aligned with the course.

Date | Section | Video Name | Timestamp | Concept | Code/Tool | Notes |
| -- | -- | -- | -- | -- | -- | -- |
2025-08-30 | 2 | Azure Databricks Workspace Setup | [1:42] | Azure DB resource group and workspace setup for all 3 environments | | |
2025-08-30 | 2 | Azure Databricks Workspace Setup | [4:30] | Same UC metastore created for all 3 workspaces in the same region  | | |
2025-08-30 | 2 | VS Code | [1:05] | Databricks extension installation in VS Code | VS Code |  |
2025-08-30 | 2 | VS Code: Tutorial | N/A | Link to VS Code guide Includes some shortcuts that can be reviewed | VS Code | [Link](https://code.visualstudio.com/docs/introvideos/basics) |
2025-09-01 | 2 | Local Java Installation | All | Spark / Java Compatibility, Mapping DB Runtime to specific Java version required | Check java version installed: java --version | - To work with Databricks locally, you will need Java installed.<br/> - Go to [Link](https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/) to check the current LTS version (check non-ML) and the version of Spark associated with it. <br/>- Once you have identified the Spark version, go to this [Link](https://community.cloudera.com/t5/Community-Articles/Spark-and-Java-versions-Supportability-Matrix/ta-p/383669) to check Spark-Java version mapping and identify the java version that needs to be installed.<br/> -In case the Spark-Java mapping is not available, go to the [Spark Homepage](https://spark.apache.org/docs/3.5.2/) for the specific version (***make sure you change the Spark version in the link***) and get the Java version for installation.<br/> - [Java Downloads Page](https://www.oracle.com/java/technologies/downloads/?er=221886)<br/> - **NOTE:** When we talk about Java Installation, we are talking about JDK installation, not JRE installation.<br/> - **Example:** Latest LTS DBR (Databricks Runtime) as of today is Databricks Runtime 16.4 LTS ML. The Databricks Runtime 16.4 LTS is powered by powered by Apache Spark 3.5.2, but we are unable to locate the corresponding Java version at [Link](https://community.cloudera.com/t5/Community-Articles/Spark-and-Java-versions-Supportability-Matrix/ta-p/383669). So, we look at [Spark Homepage](https://spark.apache.org/docs/3.5.2/) to find out in the Downloads section that "Spark runs on Java 8/11/17".<br/>  - In case you multiple versions of Java installed? - Set PATH for relevant version |
2025-09-01 | 2 | Python Installation | All | Python Installation for working with Databrciks on Local System | Python | You need to install the Python version associated with the Databricks runtime. In order to check the Python version required, go to the [Microsoft page listing the DBRs](https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/), go to the specific DBR, on the right you will find a section called 'System environment', check on it and you should be able to see the Python version listed. Please note that we just need to align the major and minor version i.e. Databricks Runtime 16.4 LTS is associated with Python: 3.12.3. Having Python: 3.12.x installed will suffice where x is a non-negative integer |
2025-09-01 | 2 | Databricks CLI | [0:15] | - Install Databricks CLI using curl<br/> - Current curl command for installation: ```curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh \| sh``` | To check Databricks CLI version:<br/> - databricks --version | - Check this [link](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/install) for installation with curl.<br/> - For Linux and macOS, if an error message states that /usr/local/bin is not writable, then run the command again with sudo i.e. add sudo infront of each piped command e.g. ```sudo curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh \| sudo sh``` |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [1:18] | Authenticate DB CLI installed locally with AZ DB workspace so that we can interact with it from our local computer | Useful commands:<br/> - databricks -h<br/> - databricks configure -h (allows us to configure CLI to interact with databricks) | - databricks configure --host <db dev workspace url> --profile DEFAULT (Command to configure 'DEFAULT' profile)<br/> - databricks auth profiles (Lists all of the profiles configured)<br/> - databricks auth describe (Gives details of how we have configured profile)<br/> - When we create a profile, a .databrickscfg file is created in our local system's user profile folder e.g. C:\Users\username\.databrickscfg. This file contains the profile details including the PAT used for authentication. |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [2:56] | PAT creation for DB Workspace  |  |  |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | [5:50] | Alternate method to open .databrickscfg file and add TEST profile  | VS Code Databricks Extension | - databricks auth describe --profile TEST (to describe a specific profile created - Generally we will created only DEV/DEFAULT profile locally since we will deploy to TEST and PROD automatically) |
2025-09-01 | 3 | Databricks CLI Workspace Configuration | N/A | Databricks CLI authentication method on Azure  |  | - [All DB CLI Auth Methods for AZ DB](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication)<br/> - [Authentication with Service Principal](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication#azure-sp-auth) |
2025-09-01 | 3 | Databricks CLI | N/A | Why you need to know the Databricks CLI? | DB CLI | Why you need to know the Databricks CLI? <br/>- Databricks Asset Bundles are a feature of the Databricks CLI. You build bundles locally, then use the Databricks CLI to deploy your bundles to target remote Databricks workspaces and run bundle workflows in those workspaces from the command line.<br/> - The CLI’s bundle commands output structured JSON and support authentication profiles, making them essential for scripting bundle workflows in CI/CD pipelines—without it, you can’t automate builds, tests or deployments of your Databricks assets |
2025-09-01 | 3 | Databricks CLI Demo | [1:11] | Create DB Cluster with CLI | - ```databricks clusters create --json @create-compute.json --profile DEFAULT``` <br/> - ```databricks clusters create --json @create-compute.json --profile TEST``` (You may get an error when executing this command using the same JSON. In this case the error is because the profile_id key in the JSON has a value associated with DEFAULT workspace and not the TEST workspace/profile ) <br/> - ```databricks clusters list --profile TEST``` (Listing clusters in TEST profile/workspace along with the cluster id and cluster state)<br/> - ```databricks clusters delete xxx-xxx-xxx``` (To TERMINATE the cluster. Here, xxx-xxx-xxx is a cluster id)<br/> - ```databricks clusters permanent-delete xxx-xxx-xxx``` (To Permanently detele the cluster using the cluster id)<br/> - ```databricks clusters permanent-delete xxx-xxx-xxx --profile TEST``` (To Permanently detele a cluster using the cluster id and profile information. By default, the cluster is searched in the workspace in the DEFAULT profile) | Create DB Cluster with CLI, also recommended method of using a JSON file as the request body (i.e. start creating the cluster fron UI and copy the JSON to invoke via DB CLI on your local system.). |
2025-09-01 | 3 | Databricks CLI Demo | [11:17] | Obtain cluster ID from Databricks UI |  |  |
2025-09-01 | 3 | Databricks CLI Demo | N/A | Databricks CLI commands reference | Databricks CLI | [Databricks CLI commands](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/commands) |
2025-09-01 | 3 | Databricks CLI Demo | N/A | Databricks REST API reference | Databricks CLI | [Databricks REST API reference](https://docs.databricks.com/api/azure/workspace/introduction) |
2025-09-01 | 3 | Databricks CLI Demo | N/A | How Databricks CLI works | Databricks CLI | When we execute a DB CLI command, the DB REST API is invoked under the hood. |
2025-09-01 | 4 | Initialising our Bundle Project | [1:05] | How to initiate Databricks Asset Bundle project and provide an overview of the bundles. | - ```databricks bundle init -h```<br/>- ```databricks bundle init --profile DEFAULT```<br/> | |
2025-09-01 | 4 | Initialising our Bundle Project | [3:07] | Project folder structure, and main configuration file. | | - databricks.yml file is the blueprint of project.Each project should only have one databricks.yml file. <br/>- When you run DAB, it takes yaml, coverts to json, and calls DB rest API.<br/>- Additional configutaions is specfiied via include key in yaml file and for the project chosen, they are in the resources folder.<br/>- scratch folder not included for production/bundle deployment. It is ignored via the .gitignore file.It isintended to be used as an exploration space. |
2025-09-01 | 4 | Initialising our Bundle Project | N/A | YAML Syntax | YAML | [YAML Syntax](https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html) |
2025-09-01 | 4 | Initialising our Bundle Project | N/A | DAB Configuration | | [DAB Configuration](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/settings) |
2025-09-01 | 4 | Initialising our Bundle Project | N/A | DAB Configuration Reference | | [DAB Configuration Reference](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/reference) |
2025-09-01 | 4 | Validating and Deploying the bundle | [0:13] | Validate Bundle before deployment | - ```databricks bundle validate``` | - By default, for root_path the Databricks CLI uses the default path of ```/Workspace/Users/${workspace.current_user.userName}/.bundle/${bundle.name}/${bundle.target}```, which uses substitutions. <br/> Here workspace.current_user.userName = databricks user name. ${bundle.name} is the name of the bundle (e.g. db_project) and ${bundle.target} is dev/test/prod etc. |
2025-09-01 | 4 | Validating and Deploying the bundle | [2:42] | Deploy Bundle | - ```databricks bundle deploy``` | root_path is where the bundle file are uploaded to in the databricks workspace. Post successful execution of the deploy command, navigate to the "root_path" in the development workspace. You will see 3 folders:<br/> - artifacts: stores any build artifacts such as Python packages. So for now it's empty because we don't have any build artifacts. <br/> - files: Stores all the files. So this is all the code files. Notice how we have the resources and the fixtures directories as well as the Databricks YAML and the README.md file.But we have not got the other folders like scratch and .databricks (they were excluded using the .gitignore). <br/> - state: used to track metadata about your previous deployments. When you redeploy a bundle, the tool compares your current configuration to the previous state and incrementally updates only the changed assets and then refreshes the state file to ensure that everything in your Databricks workspace stays in sync. |
2025-09-01 | 4 | Validating and Deploying the bundle | [5:06] | Adding a new folder to the bundle. |  |  |
2025-09-01 | 4 | Validating and Deploying the bundle | [6:45] | For viewing summary but not doing any validation. | - ```databricks bundle summary```  |  |
2025-09-01 | 4 | Validating and Deploying the bundle | [7:00] | Adding a custom root_path for dev. |  |  |
2025-09-01 | 4 | Deploying the bundle to different targets | [0:20] | Deploying the bundle to different targets | ```databricks bundle deploy --target test``` | Update the databricks.yml file to define other targets and update the root_path and workspace host as shown. Update the .databrickscfg file to include information about any missing targets. Earlier, when we executed ```databricks bundle deploy``` , we deployed only the 'dev' target because in databricks.yml, under targets->dev, we set ```default: true```. Please note that the 'mode' for both 'test' and 'prod' targets is 'production' while that of the 'dev' target is 'development' |
2025-09-01 | 4 | Deploying the bundle to different targets | [5:40] | Attempt to deploy dev target in the Shared path | | - We get an error. The root path must contain the current username to ensure uniqueness when using the mode as 'development'. So when the mode is 'development', it does not allow you to deploy to a non-unique location. You must have your username in the deployment route path. So, picture this scenario when multiple developers are making changes in their development cycle, the last thing you want for them to do is keep overwriting each other's changes. So that's why when the mode for the deployment is set to development, then the path must contain the username. <br/> - For 'test' and 'prod' targets, we can set the mode to 'production' because we don't have a requirement to deploy to a unique location, because all assets for testing and production should be deployed to a centralized, shared location where the changes have been thoroughly tested and approved. So the deployments to test and prod would normally be more controlled than deployments to Dev. <br/> - **Best practice to remove the root_path from dev target (with mode: development) and deploy it in the default unique location.** <br/> - **In a real world project from your local workspace, you should only be deploying to the dev target. But for the purposes of this demo, we wanted to show you how you can deploy to multiple targets such as test and prod. This is so you'll understand later on in this course how to set up a CI CD pipeline that will deploy to test and prod automatically.** |
2025-09-01 | 4 | Simple Workflow Job Demo and Deployment Modes | [0:06] | How to define and deploy a job via Databricks Asset bundles. |  | - The job name has a prefix against the job name. Let's discuss why we have this prefix here. But before we do that, let's actually deploy this to the test workspace. So currently we don't have any workflows here. So now let's deploy it to the test workspace. So for the test target we're deploying the bundle in the shared folder. We're not deploying it in our workspace folder. In test target, if we look at the job name, we see that, the workflow doesn't have a prefix. So this behavior is controlled by the 'mode' key in the target specification. In our Databricks YAML setup you'll recall that for the target of 'dev' we've specified the mode is 'development', but the mode is 'production' for 'test'. So when the mode is set to development, a prefix which includes the username is automatically added to the job name. This ensures that when multiple developers deploy their jobs in the development environment, their deployments remain unique and don't overwrite each other. In contrast, this prefix isn't required when the mode is set to production. When the mode is production, we aim to have a single consolidated workflow for either test or production use cases.<br/> - Further details can be found here: [Databricks Asset Bundle deployment modes \| Development mode](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/deployment-modes#development-mode)  |
2025-09-01 | 4 | Simple Workflow Job Demo and Deployment Modes | [6:47] | Custom presents to define prefix when deploying in 'development' mode. |  | - [Custom presets](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/deployment-modes#custom-presets) <br/> - Please note that the custom prefix must be unique when mode='development' for a target. |
2025-09-01 | 4 | Modularising your configuration files | [0:27] | The configuration files are added directly to this databricks.yml file. Now, having all of the configuration in a single file can get quite complex. If we had multiple 'resources', then the file can get pretty long and it can get difficult to manage. So it's a good idea to modularize the configuration. |  | Notes |
2025-09-01 | 4 | Destroying a bundle | [0:00] | Destroying a bundle | - ```databricks bundle destroy``` | The Databricks bundle destroy command permanently deletes a bundles previously deployed assets such as jobs, pipelines, and artifacts. This action cannot be undone in the dev workspace. |
2025-09-01 | 4 | How Databricks tracks the state of your Deployments  | [0:35] | Deployment State Tracking |  | - How Databricks keeps track of your various deployments and their state. So the .databricks directory in the root folder of your bundle acts as a manifest. It contains files that store detailed logs and tracks the state of every asset in your bundle. The Databricks CLI will compare the current state of your local assets with the last known deployed state recorded in this directory. And if the file remains unchanged since the last deployment, then the tool will recognize that there's no need to redeploy it, which speeds up the process by updating only the assets that have changed. <br/> - In order to force a full deployment, delete the .databricks folder and deploy again. |
2025-09-01 | 5 | Creating Databricks Compute Cluster for Development | [0:34] | How to use Databricks Asset bundles for your local development purposes. | | - Cluster created/being used needs to be Unity Catalog enabled.<br/> - The cluster is created from the UI in this course.  |
2025-09-01 | 5 | Integrating VS Code with Databricks compute | [0:10] | How to connect to development cluster from Visual Studio Code so that we can access this cluster remotely from our local development environment. | Databricks VS Code Extension | - We connect to the cluster using the Databricks VS Code extension. Go to the extension, click on 'Select a cluster' and then click on the desired cluster name that is required to be used for development workloads. You would also have options for Serverless and for Cluster creation. Once selected, we have options to Stop and Start the cluster. We also get a link to open the Cluster in DB workspace.<br/> - If you open a notebook in VS Code, you will find a databricks symbol on the top-right corner of the notebook. If you click on that, it will submit the notebook as a Databricks job and we should see the output shortly. <br/>- We can submit notebooks to the spark cluster. We will be able to view the output in VS Code. We can also go to Workflows -> Job runs in the databricks workspace to check the execution of the job. <br/> - The disadvantage is that we cannot execute cell by cell on databricks spark cluster. The way we can configure this to run individual cells by connecting to the Databricks cluster is by using Databricks Connect. |
2025-09-01 | 5 | Python Virtual Environments | N/A | Python Virtual Environments | Python | - In the next lecture we will use Python virtual environments to isolate and install all local-development dependencies—like PySpark and Databricks Connect. <br/>- If you’re new to virtual environments, check out this [YouTube tutorial](https://www.youtube.com/watch?v=ReAy0oKWX7o) for a quick primer. |
2025-09-01 | 5 | Installing Databricks Connect | [0:38] | Run individual cells of a notebook on remote Databricks cluster | Databricks Connect | - To be able to run individual cells using the remote cluster, we need to install the Databricks Connect package. It's a requirement to install Databricks Connect in a virtual environment because otherwise it conflicts with local PySpark installations. So in our project let's create a virtual environment. <br/> - Make sure you select the version of Python that's compatible with your Databricks runtime. <br/> - ```python -m venv .venv_dbc``` and ```source .venv_dbc/bin/activate``` run locally on Linux. <br/> - Okay, so now that we're in the virtual environment so the virtual environment is active let's install Databricks Connect. So we want to install the same version of Databricks Connect as Databricks Runtime. If you go to the clusters page in the DB workspace, you will find that Databricks cluster is of runtime version 15.4. So that's the version that we'll install. <br/> - For DBR 15.4 we will execute ```pip install databricks-connect==15.4``` <br/> - Now to go to the notebook and change the kernel on this notebook to the created virtual environment. <br/> - Create a cell with the code ```spark.sql("select 1").show()``` and run the cell. We will get an output. So we were able to run this command directly through the Databricks remote cluster. |
2025-09-01 | 5 | Installing Databricks Connect | [5:05] | Run a python script on remote Databricks cluster | Databricks Connect | - Databricks connect should be set up first<br/> - Create a python script (.py file) and add the code ```spark.sql("select 1").show()``` and save it.<br/>- On the top right side of the file, click on the Databricks icon which leads to 4 options being shown, one of which is ![alt text](images\databricks-connect-python-script-run-option.png). <br/>- Click on the highlighted option (Run Current file with Databricks connect).<br/> - It will ask you to select a Python environment. If it does then just select this environment which is which is the virtual environment we created. And to ensure it's selected what you can do is you can go to View Command Palette and then you can type 'Python: Select Interpreter'. And then you can click and just select the created virtual environment as your interpreter.  <br/> - Proceed to "Run Current file with Databricks connect", it should run. |
2025-09-01 | 5 | Creating a DatabricksSession to run scripts on the terminal | All | Run python scripts on remote DB Cluster in VS Code | Databricks Connect | - We can submit a python script to remote DB Cluster via the DB button on top right side. But if we wanted to run this script as a Python file directly in our terminal, then Databricks extension features are not supported. So in that instance, we'd need to initiate a Databricks Connect session in the python script. There will be some scenarios where we need to run scripts in the terminal. Some of these examples would be CI CD workflows or automation scripts. So in these instances we need to explicitly create a Databricks session. To be clear, a Databricks session is not the same as a spark session. A spark session is provided by PySpark, but Databricks session carries settings for authenticating and communicating with a remote Databricks cluster. So Databricks Connect directs your operations to the cluster. <br/> - So in this script, we need to create a Databricks session via the Databricks Connect package and assign that to this spark variable. ```from databricks.connect import DatabricksSession``` <br/> - This Databricks session is not the same as from ```pyspark.sql import SparkSession``` . This is not the same spark session is provided by PySpark, but the Databricks session carries settings for authenticating and communicating with a remote Databricks cluster. <br/> ```from databricks.connect import DatabricksSession``` <br/>```spark = DatabricksSession.builder.getOrCreate()``` <br/>```spark.sql("select 1").show()``` <br/> - So this Databricks Session Builder gets all the relevant configurations such as the Databricks Workspace host, the token and the cluster ID from Databricks Configuration file. Since we haven't actually configured the cluster ID in Databricks configuration file, add cluster_id under DEFAULT databricks profie in the .databrickscfg file. <br/> - Now the script runs from terminal e.g. ```python temp.py``` (provided the virtual environment is active) OR clicking on the 'Run' button (provided we have selected the current .venv python as the selected Python Interpreter using Ctrl+Shift+P) <br/> - We can also provide the cluster id in the DatabricksSession as: <br/> ```from databricks.connect import DatabricksSession``` <br/> ```spark = DatabricksSession.builder.remote(cluster_id='xxx-xxx-xxx').getOrCreate()``` <br/>```spark.sql("select 1").show()```<br/> - For deeper understanding, refer to [Link](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect/python/advanced) |
2025-09-01 | 5 | VS Code: Selected Python Interpreter vs Active Virtual Environment | N/A | Selected Python Interpreter vs Active Virtual Environment | Databricks Connect | - When you select a Python interpreter in Visual Studio Code, you're telling the editor which Python executable to use for running your code. You can select the Python interpreter by going on 'View' -> 'Command palette' and then typing 'Python: Select Interpreter'. <br/> - So the interpreter can be from the global system or from a virtual environment. <br/> - Let us select the virtual environment created (.venv_dbc) <br/> - Since we selected the interpreter from our project's virtual environment then Visual Studio Code will also be aware of the packages installed in that environment. So when you open a script such as this Python file and run it using the Visual Studio Codes UI features such as display button, then it will automatically use the selected Python interpreter. So notice currently in the terminal I don't have the virtual environment as active. So if I click on this play icon despite it not being active, it activates that virtual environment and it runs the script on the remote Databricks cluster because that was my selected interpreter. <br/> - On the other hand, when you run commands directly in the terminal, Then it's the active virtual environment that matters. So if I want to run this script from this terminal, then I should ensure that this virtual environment is active. i.e. execute ```source .venv\bin\activate``` to activate the virtual environmet. So now that this virtual environment is active I can run this by typing ```python file.py```. <br/> - Best practice - Activate the created virtual environment in the terminal and then select the same environment via 'View' -> 'Command palette' -> 'Python: Select Interpreter'. This will guarantee consistent behavior whether you run scripts via the Visual Studio Code interface, such as this play button or via the terminal. |
2025-09-01 | 5 | Exclude your local venv from Git & Deployments | N/A | Exclude your local venv from Git & deployments | .gitignore | We’ve created a local virtual environment named .venv_dbc. To prevent it from being committed or bundled, add the following to your .gitignore file: ```.venv_*/``` This pattern matches any directory starting with .venv (including .venv_dbc), ensuring you never ship large environment folders—avoiding huge deploy artifacts or timeout issues. **Note there is an existing line with the pattern .venv/ but this will not suffice as it does not match the pattern .venv_dbc** |
2025-09-02 | 6 | Course Project Overview | Skip | Project Overview | N/A | So initially I'll develop this solution using Databricks notebooks and run it as a job. I'll then refactor it into Python scripts, which I'll also run as a Databricks job. But as Python tasks. And then lastly, I'll convert the logic into Delta live tables pipelines. |
2025-09-02 | 6 | Workspace Setup | All / Skip | Create Course Catalog/Schemas/Volumes | Databricks | Since we have a shared metastore, you can create all catalogs from a single workspace. Create 'catalog_{target_name}' catalogs, where 'target_name' = ['dev','test',''prod']. Within each catalog, create 4 schemas - 00_landing, 01_bronze, 02_silver and 03_gold. Within each 00_landing schema, create a volume called 'source_citibike_data' and upload the CSV file in resources to that volume. |
2025-09-02 | 7 | Codebase Structure | [1:20] | Structure of Project | N/A | - Added benefit when developing with Databricks Connect is that the Databricks runtime automatically includes your project's directories, such as this src subfolder in the Python path, i.e. sys.path. That means that when you run your code via Databricks Connect, Python knows where to look for your modules without requiring you to manually add the project route to the Python path. <br/> - So, for example, if I wasn't using Databricks Connect, I'd have to include the following code (**check in video**). This code snippet obtains the current working directory from the OS module, so this will be the current working directory for this notebook. And then it computes the project root directory. And finally it appends the project root directory to the Python's module Searchpath using sys.path.append. So in other words, it makes sure that the modules in the project root can be imported by your notebook, but this is done automatically when you run your code via a remote cluster using Databricks Connect. So we don't need to use this code when we're using Databricks Connect. |
2025-09-02 | 7 | Bronze Notebook Walkthrough | All | Code walkthrough | N/A | - Make sure general purpose cluster is active and venv is selected as python interpreter and activated in terminal. <br/> - We will parameterize the catalog names in code later <br/> - read up on create_mapping function <br/> - Just execute code. Ensure any display/show cells are removed after going through notebooks. |
2025-09-02 | 7 | Silver Notebook Walkthrough | All | Code walkthrough | N/A | - Comments similar to Bronze Notebook Walkthrough |
2025-09-02 | 7 | Gold Notebook Walkthrough | All | Code walkthrough | N/A | - Comments similar to Bronze Notebook Walkthrough <br/> - **So that's the ETL logic from end to end. We're now ready to create a Databricks job that processes the bronze, silver, and gold notebooks as tasks and will deploy that to the dev target so that will be deployed to this workspace.** |
2025-09-02 | 7 | Local Module Imports in Databricks | All / Watch | Local Module Imports in Databricks | ```import sys```<br/>```import os```<br/>```current_dir = os.getcwd()```<br/>```project_root = os.path.abspath(os.path.join(current_dir,'..','..','..'))```<br/>```sys.path.append(project_root)``` | When using databricks connect on VS Code, the local project directory gets appended to sys.path automatically. But, if we deploy to DB Workspace and run the notebooks, the same does not happen and we are unable to import the local module functions. Hence, we add a code at the start of the notebook(s) which adds the project folder path to the sys.path. |
2025-09-02 | 7 | Job configuration and Dev deployment | All / Watch | Use DAB resorces for deploying job |  | - We'll now create a Databricks job that runs each notebook in the bronze, silver, and gold layers as tasks. So we can either add the job definition in this databricks.yml file, or in its own standalone .yml file in the resources folder. And the reason we can do that is because of this includes flag, where we've included this resources folder path. This means that we can have additional configuration files in this path, and it's a good idea to modularize your code rather than have everything in one big configuration file. So that's what we'll do.<br/> - Video demostrates how the job's .yml file needs to be created by leveraging the UI i.e. the specific edits that are required.  |
2025-09-02 | 7 | Configuring, Deploying and Running the ETL Job | All / Watch | Dynamic value references usage in databricks workflows and how to configure asset bundle resources with base parameters. |  | - How to configure base parameters for your notebook tasks? If you recall in the bronze and the silver notebooks, we use this create_map function. And right now we're using placeholder values (pipeline_id, run_id, task_id and processed_timestamp). But we want to pass in dynamic value references from the jobs tasks directly into the notebook, which we can then use to replace these placeholders. <br/> - This [link](https://learn.microsoft.com/en-us/azure/databricks/jobs/dynamic-value-references) gives you information about the dynamic value references we can use in our Databricks job tasks. |
2025-09-02 | 7 | Regular Variable with Manual Override | [1:19] | Bundle Variables | ```databricks bundle deploy --target test --var="catalog=citibike_test"``` <br/> ```databricks bundle deploy --target test --var="catalog=citibike_test, foo=bar"``` (Multiple variables example) | - Using bundle variables to parameterize the catalog names (citibike_dev/citibike_test/citibike_prod) that change based on the target (dev/test/prod) <br/> - So the way we can make this dynamic is by creating a bundle variable and referencing the variable in our notebooks. And the variable should change dynamically based on whether we're deploying to dev, test or prod.  |
2025-09-02 | 7 | Regular Variable with Target Override | [0:29] | Rather than doing a manual override of the bundle variables in the terminal, we can actually set target specific variable overrides directly in the configuration files (databricks.yml). |  | - In the previous lecture, we defined a top level variable in the Databricks configuration file and the variable was catalog. The default value for that is city bike underscore dev. But we can actually override this top level variable by adding the same variable name to specific targets, such as test and prod. And if we give it a different value, then whatever the variable value is in the target will take precedence. |
2025-09-02 | 7 | Complex Variables | [0:35] | Complex Variables |  | - In this lecture, we are going to demonstrate how you can use complex variables. A complex variable in Databricks asset bundles is a variable that contains structured nested data rather than a single scalar value. Instead of just holding a simple string or number, it holds a mapping of key value pairs. So we can capture a full configuration block. This enables you to centrally manage and dynamically override multiple related parameters at once during deployment. We demonstrate this by using complex variables to store cluster configurations. |
2025-09-02 | 7 | Complex Variables | [5:06] | Configuring differet clusters for different tasks of the same job using complex variables  |  | - Now, just for demonstration purposes, what if we wanted to use one specific type of cluster for the bronze and silver notebook tasks? So rather than having a standard ds3 v2 node type, I want to use a standard underscore F4 node type just for these two tasks. And then I can keep these two tasks the same using the same ds3 v2 single node cluster. We can achieve that by adding an additional cluster in the job configuration. |
2025-09-02 | 7 | Lookup Variables (+Documentation for variables) | [1:00] | Lookup Variables | N/A | - [Substitutions and variables reference](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/bundles/variables) <br/> - So a lookup variable can retrieve a named object's Objects ID so you can see some of the applicable objects here. So alerts cluster policies clusters and so on. So under the variable you just give it a variable name. And then you provide this lookup key. You then specify the object type. So for example is it a cluster. Is it an alert. Is it a pipeline. And then a colon. And for the value the name of that object. So just by providing the name of that object it will then retrieve the object's ID. <br/> - Consider for knowledge only. |
2025-09-13 | 8 | Handling Parameters with Python Script Tasks | [0:40] | Convert ETL Notebooks to .py scripts and create corresponding job for execution | - In "Parameters" of UI: ```["{{job.id}}", "{{job.run_id}}"]``` <br/> - In Python script being invoked <br/> ```import sys``` <br/> ```job_id = sys.argv[1]``` <br/> ```run_id = sys.argv[2]``` <br/> - Notice how the index position starts from 1 and not 0. <br/> - This is because the index position 0 contains the path to the script (within Databricks) being executed in the task. <br/> | - The parameters in Python Script Task is handled differently vs Notebook Tasks in Databricks Jobs. <br/> - Same approach to create a job with Python script using Databricks UI and the copying the yaml to add in asset bundles. <br/> - For parameters, you need to provide a list of values in the job. Since keys are not allowed in a list, we need to be aware of the sequence of different parameters and retrieve them using sys.argv[position] at the start of the Python script invoked in the job task. |
2025-09-13 | 8 | Walkthrough of Refactored Code | All | Walkthrough of Refactored Code |  | - Here we create a nw .job.yml file in the resources folder that contains the configuration for the ETL job using Python scripts. <br/> - Light skimming of video recommended. |
2025-09-13 | 9 | Creating a Python Wheel (.whl) | [3:11] | Package your moduldes with wheel to install it with pip or attach it to your cluster so that you can use those modules in your code withouting adding any additional paths to system ```PATH```. |   | - Installation of two packages required ```setuptools``` and ```wheel``` <br/> - create a python file in the project root called ```setup.py``` <br/> - Call setup within the file with required arguments as shown in the video. <br/> - Run ```python setup.py bdist_wheel``` <br/> -Twi directories are created ```buiild``` (contains intermediate files created by setuptools during the build process) and ```dist``` (which contains the wheel). <br/> - Copy the relative path of the .whl file and run the command ```pip install dist/wheel_name.whl``` <br/> - Now, even without adding the root directory to system PATH, the custom modules can be imported in the python scripts. |
2025-09-13 | 9 | Installing the wheel on a Databricks cluster | [0:14] | Installing the wheel on a Databricks All purpose cluster via UI | Databricks | - Upload .whl file to User folder in Databricks workspace <br/> - for an All Purpose Cluster, click on it, go to libraries tab, click on "Install new" to the right <br/> - in the dialog box that pops up select "File path/ADLS" and "Python Package", provide the path to the .whl file located in your workspace (use the Copy URL path option) <br/> - click "Install" <br/> - start/restart the cluster <br/> - open a notebook <br/> - attach the cluster <br/> - import the module <br/> - it should work!   |
2025-09-13 | 9 | Installing wheel packages in your Databricks jobs | [0:21] | We already know that we can attach the wheel to an all purpose compute cluster. But when we run our jobs, we typically run these on a job compute cluster, which is created on the fly to ensure that our wheel file is installed on that job cluster, we need to add it directly to the task definition. | Databricks | - Upload .whl file to User folder in Databricks workspace <br/> -When you are creating a job, just below the option "Cluster" (were a Job cluster is selected), there is an option called Dependent Libraries <br/> - Click "+Add", go to libraries tab, click on "Install new" to the right <br/> - in the dialog box that pops up select "File path/ADLS" and "Python Package", provide the path to the .whl file located in your workspace (use the Copy URL path option) <br/> - click "Add" <br/> - Create the task <br/> - Review Yaml <br/> - In the yml, you will find that a "libraries" entry that contains the .whl path  |
2025-09-13 | 9 | Installing wheel packages in your Databricks jobs | [1:31] | Update asset bundles to specify .whl installation on job clusters | Databricks |  - wheel path to use when updating the *.job.yml file is ```../dist/*.whl``` - By the way, you might have noticed the "dist" folder has randomly disappeared. This can happen if you change the configurations that's expected. So the Databricks CLI will delete this directory when any wheel related settings are updated. This is to encourage a fresh rebuild and to avoid using stale wheel files. So when we deploy our bundle, the Databricks CLI will automatically create this real file. As long as it's referenced somewhere in the bundle config. So we don't actually manually need to build the file like we did previously. So we built the file by typing ```python setup.py bdist_wheel```. Well, this is actually done automatically for us. The reason it's done automatically for us is because the Databricks CLI will identify that we are referencing a wheel file here, so it will know to leverage the setup.py file and automatically build the wheel file. The wheel file is also deployed to the bundles target location, and that's why we can reference it using this relative path. <br/> - In the scripts/notebooks we remove adding the module path tp system PATH and correct the imports by removing ```src.``` from the imports i.e. ```from src.citibike import utils``` becomes ```from citibike importutils``` <br/> - deploy the bundle and run the job. <br/> - to check if the wheel has been installed, open the notebook/script where the library is being used and on the right you should find an option called "View run libraries" to check. |
2025-09-13 | 9 | Python Wheel Task | All / Review | Using a Python wheel task to install .whl in a clsuter during a workflow execution. |  | Review thoroughly |